```{r setup_sequences, include=FALSE}
rm(list = ls()) ; invisible(gc()) ; set.seed(42)
library(knitr)
library(kableExtra)
if(knitr:::is_html_output()) options(knitr.table.format = "html") 
if(knitr:::is_latex_output()) options(knitr.table.format = "latex") 
library(parallel)
library(tidyverse)
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = T)
theme_set(bayesplot::theme_default())
opts_chunk$set(
  echo = F, message = F, warning = F, fig.height = 6, fig.width = 8,
  cache = T, cache.lazy = F)
path <- "../../../data/Eschweilera_Paracou"
pathCluster <- "~/Remotes/genotoul/work/PhD/data/Eschweilera_Paracou"
```


```{r}
individuals <- googlesheets::gs_title("Parvicapture") %>% 
  googlesheets::gs_read("Extraction") %>% 
  dplyr::select(IdGenetic, Genus, Species, Plot, SubPlot, TreeFieldNum) %>% 
  unique() %>% 
  na.omit()
```

# Sequences

We received demultiplexed libraries from sequencing and process to their quality check, trimming, mapping, and variant calling.

* __Counts__: numbr of reads per forward and reverse libraries
* __Quality__: we used multiqc to combined fastqc iuputs for every library (1002 for forward and reverse individuals) and check sequences quality and GC content
* __Trimming__: we trimmed sequences removing bad quality and adaptors sequences
* __Reference__: reference gathering from targets
* __Individuals merging__: repeated individuals were merged
* __Mapping__: we mapped every individual in pair end with `bwa mem` on the reference
* __Variant calling__ Run the `HaplotypeCaller` on each sample's BAM files to create single-sample gVCFs using the `.g.vcf` extension for the output file
* __Data aggregation__ Aggregate the GVCF files and feed in one GVCF with `GenomicsDBImport` to be genotyped
* __Joint genotyping__ Run `GenotypeGVCFs` on all of them together to create the raw SNP and indel VCFs that are usually emitted by the callers

## Counts

We have a big heterogeneity of sapmle representativity (56 000 folds), and only 61% of samples have more than 66 667 sequences (ca 1M targets / 150 bp * 10X). Pooling individuals the heterogeneity drop to 14 000 folds and 78% of individuals have more than 66 667 sequences (ca 1M targets / 150 bp * 10X). We can still hope that individual repetitions will help recover individuals with a few sequences, but we should be ready to lose many individuals in downstream analyses !

```{r seqcounts, fig.cap="Sequence counts per sample (individuals x repeats x lane x strand) and individuals."}
gSample <- lapply(list.files(file.path(path, "Sequences", "quality"), full.names = T, pattern = "general_stats"), read_tsv) %>% 
  bind_rows() %>% 
  mutate(Lane = as.factor(ifelse(grepl("L001", Sample), 1, 2))) %>% 
  dplyr::rename(Reads = `Input Reads (absolute number)`) %>% 
  ggplot(aes(reorder(Sample, Reads), Reads, fill = Lane)) +
  geom_col() +
  geom_hline(yintercept = 66667, linetype = "dashed", col = "red", size = 1.2) +
  coord_flip() +
  scale_y_log10() +
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(), 
        axis.line.y =element_blank(), 
        axis.ticks.y = element_blank()) +
  ylab("Total sequences")
gInd <- lapply(list.files(file.path(path, "Sequences", "quality"), full.names = T, pattern = "general_stats"), read_tsv) %>% 
  bind_rows() %>% 
  dplyr::rename(Reads = `Input Reads (absolute number)`) %>% 
  separate(Sample, c("ID1", "ID2", "Sample", "ID3", "Lane", "Strand", "ID4"), sep = "_") %>% 
  dplyr::select(Sample, Strand, Reads) %>% 
  separate(Sample, c("Id", "Ind", "Lane"), sep = "-") %>% 
  group_by(Ind) %>% 
  summarise(Reads = sum(Reads)) %>% 
  ggplot(aes(reorder(Ind, Reads), Reads)) +
  geom_col() +
  geom_hline(yintercept = 66667, linetype = "dashed", col = "red", size = 1.2) +
  coord_flip() +
  scale_y_log10() +
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(), 
        axis.line.y =element_blank(), 
        axis.ticks.y = element_blank()) +
  ylab("Total sequences")
cowplot::plot_grid(gSample, gInd, labels = c("Samples", "Individuals"))
```


## Quality

Sequences quality is relatively good as the Phred score are above 25 for every bases on all positions across all sequences, except a few between 15 and 25 for intermediate positions !


## Trimming

We listed all libraries in a txt files and trimmed all libraries with `trimmomatic` in pair end (`PE`) into paired and unpaired compressed fastq files (`fq.gz`). We trimmed the adaptor (`ILLUMINACLIP`) of our protocol (`TruSeq3-PE`) with a seed mismatches of 2 (mismatched count allowed), a threshold for clipping palindrome of 30 (authorized match for ligated adapters), a threshold for simple clip of 10 (match between adapter and sequence), a minimum adaptor length of 2, and keeping both reads each time (`keepBothReads`). We trimmed sequences on phred score with a minimum of 15 in sliding window of 4 (`SLIDINGWINDOW:4:15`) without trimming the beginning (`LEADING:X`) or the end (`TRAILING:X`). Trimming resulted in 98.84% of paired reads but 2 libraries had more than 80% of loss of reads, besides the rest was below 2%.

```{r libraries, eval=F, echo=T}
lapply(list.files(file.path(path, "Sequences", "quality"), full.names = T, pattern = "general_stats"), read_tsv) %>% 
  bind_rows() %>% 
  dplyr::rename(Reads = `Input Reads (absolute number)`) %>% 
  mutate(Sample = gsub("_R[12]_001", "", Sample)) %>%
  unique() %>% 
  dplyr::select(Sample) %>% 
  write_tsv(path = file.path(path, "Sequences", "libraries.txt"), col_names = F)
```

```{bash trimming, eval=F, echo=T}
for file in $(cat libraries.txt); do echo "module load bioinfo/Trimmomatic-0.36 ; java -jar \$TRIM_HOME/trimmomatic.jar PE ~/save/"$file"_R1_001.fastq.gz ~/save/"$file"_R2_001.fastq.gz paired/"$file"_R1_paired.fq.gz unpaired/"$file"_R1_unpaired.fq.gz paired/"$file"_R2_paired.fq.gz unpaired/"$file"_R2_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads SLIDINGWINDOW:4:15"; done > trimming.sh
sarray -J trim -o out/trimmed_%j.out -e out/%j.err -t 48:00:00 --mail-type=BEGIN,END,FAIL trimming.sh
for file in $(ls paired); do echo "zcat paired/$file | echo $file\"   \"\$((\`wc -l\`/4)) >> paired_stat.txt" ; done > paired.stats.sh
sarray -J paired -o out/paired_%j.out -e out/%j.err -t 48:00:00 --mail-type=BEGIN,END,FAIL paired.stats.sh
for file in $(ls unpaired); do echo "zcat unpaired/$file | echo $file\"   \"\$((\`wc -l\`/4)) >> unpaired_stat.txt" ; done > unpaired.stats.sh
sarray -J unpaired -o out/unpaired_%j.out -e out/%j.err -t 48:00:00 --mail-type=BEGIN,END,FAIL unpaired.stats.sh
```

```{r trimmingStat, fig.cap="Trimming results."}
lapply(list.files(file.path(path, "Sequences", "quality"), full.names = T, pattern = "general_stats"), read_tsv) %>%
  bind_rows() %>% 
  dplyr::rename(Reads = `Input Reads (absolute number)`) %>% 
  mutate(Sample = gsub("_001", "", Sample)) %>% 
  mutate(R = str_sub(Sample,-2,-1)) %>% 
  left_join(read_delim(file.path(path, "Sequences", "trimming", "paired_stat.txt"), col_names = c("Sample", "Paired"), delim = " ") %>% 
              mutate(Sample = gsub("_paired.fq.gz", "", Sample))) %>% 
  left_join(read_delim(file.path(path, "Sequences", "trimming", "unpaired_stat.txt"), col_names = c("Sample", "Unpaired"), delim = " ") %>% 
              mutate(Sample = gsub("_unpaired.fq.gz", "", Sample))) %>% 
  mutate_at(c("Paired", "Unpaired"), as.numeric) %>% 
  # summarise(Reads = sum(Reads), Paired = sum(Paired), Unpaired = sum(Unpaired))
  mutate(PctUnpaired = (Unpaired/Reads)*100) %>% 
  mutate(PctRemoved = (1-Paired/Reads)*100) %>% 
  select(Sample, R, PctUnpaired, PctRemoved) %>%
  reshape2::melt(id.vars = c("Sample", "R")) %>% 
  ggplot(aes(Sample, value, fill = R)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_y_sqrt() +
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(), 
        axis.line.y =element_blank(), 
        axis.ticks.y = element_blank()) +
  facet_wrap(~ variable, scale = "free") +
  ylab("In percentage of reads after trimming") +
  scale_fill_discrete("", labels = c("forward", "reverse")) +
  ylim(0,2) +
  ggtitle("2 354 614 536 raw sequences", "2 327 210 632 (98.84%) trimmed and paired & 2 979 763 (0.13%) trimmed unpaired")
```

## Reference

The reference is made of the neutral and functional targets designed for the capture.

```{bash reference, eval=F, echo=T}
cat functional.targets.20190503.fasta neutral.targets.20190503.fasta > reference.fasta
module load bioinfo/deconseq-standalone-0.4.3
deconseq.pl -f reference.fasta -out_dir deconseq_bact -dbs bact
cat deconseq_bact/*_cont.fa | grep ">"
>TRINITY_DN60869_c0_g1_i1 len=401 path=[1:0N400] [N1, 1, N2]
>TRINITY_DN72082_c0_g1_i1 len=380 path=[1:0N379] [N1, 1, N2]
>TRINITY_DN4622_c0_g1_i1 len=316 path=[1:0N178 157:179N315] [N1, 1, 157, N2]
>test003_nuc_372107
>test003_nuc_584012
deconseq.pl -f reference.fasta -out_dir deconseq_default
cat deconseq_default/*_cont.fa | grep ">"
>TRINITY_DN24848_c1_g1_i1 len=475 path=[1:0N267 454:268N290 270:291N303 283:304N324 304:325N333 313:334N474] [N1, 1, 454, 270, 283, 304, 313, N2]
>test001_nuc_1073852
>test001_nuc_433258
>test002_nuc_341176
>test002_nuc_959182
>test003_nuc_319402
>test003_nuc_372107
>test003_nuc_427809
>test003_nuc_533716
>test003_nuc_584012
>test003_nuc_691972
>test007_nuc_1025458
>test007_nuc_709019
>test008_nuc_660309
```


## Merging

Repeated libraries were merged to increase their information before mapping and variant calling.

```{bash merging, eval=F, echo=T}
cat libraries.txt | sed -e 's/[[:digit:]]*_[[:alnum:]]*_[[:digit:]]*-//' | sed -e 's/-[[:alnum:]]*_[[:alnum:]]*_[[:alnum:]]*//' | sort | uniq > individuals.txt
for ind in $(cat individuals.txt); do echo "cat paired/*-$ind*R1_paired.fq.gz > paired.merged/\"$ind\"_R1_paired.fq.gz"; done > mergingR1.sh
for ind in $(cat individuals.txt); do echo "cat paired/*-$ind*R2_paired.fq.gz > paired.merged/\"$ind\"_R2_paired.fq.gz"; done > mergingR2.sh
sarray -J R1merge -o out/R1merge_%j.out -e out/R1merge_%j.err -t 48:00:00 --mail-type=BEGIN,END,FAIL mergingR1.sh
sarray -J R2merge -o out/R2merge_%j.out -e out/R2merge_%j.err -t 48:00:00 --mail-type=BEGIN,END,FAIL mergingR2.sh
```

## Mapping

We mapped every libraries in pair end with `bwa mem` on the targets defined for the capture. We had 80.8% of individuals with more than a third of mapping reads, which is expected as we used only the targets as reference, dropping thus off-targets reads (ca two third for *Symphonia*).

```{bash mapping, eval=F, echo=T}
for ind in $(cat individuals.txt); do echo  "module load bioinfo/bwa-0.7.15 ; module load bioinfo/picard-2.14.1 ; module load bioinfo/samtools-1.4 ; bwa mem -M -R \"@RG\tID:$ind\tSM:$ind\tPL:HiSeq4K\" -t 2 reference/reference.fasta trimming/paired.merged/"$ind"_R1_paired.fq.gz 	trimming/paired.merged/"$ind"_R2_paired.fq.gz > mapping/sam/"$ind".sam ; java -Xmx4g -jar \$PICARD SortSam I=mapping/sam/"$ind".sam O=mapping/bam/"$ind".bam SORT_ORDER=coordinate ; rm mapping/sam/"$ind".sam ; samtools index mapping/bam/"$ind".bam"; done > mapping.sh
sarray -J mapping -o out/mapping_%j.out -e out/mapping_%j.err -t 48:00:00 --mem=10G --cpus-per-task=2 --mail-type=BEGIN,END,FAIL mapping.sh
for file in $(ls bam*/*.bam); do echo 'module load bioinfo/samtools-1.4 ; samtools flagstat '$file' | echo '$file'"   "$(grep "mapped (") >> readsMappingStat.txt'; done > mappingStat.sh
sarray -J mapping -o out/%j.out -e out/%j.err -t 1:00:00 --mail-type=BEGIN,END,FAIL  mappingStat.sh
```

```{r mappingStat, fig.cap="Mapping result"}
read_delim(file.path(path, "Sequences", "mapping", "readsMappingStat.txt"),
           delim = " ", col_names = c("Library", "readsMapped", "+", "0", "mapped", "percentage", ":", "N/A")) %>% 
  dplyr::select(Library, readsMapped, percentage) %>% 
  mutate(Library = gsub("bam/", "", Library)) %>% 
  mutate(Library = gsub(".bam", "", Library)) %>% 
  mutate(percentage = gsub("(", "", percentage, fixed = T)) %>% 
  mutate(percentage = gsub("%", "", percentage, fixed = T)) %>% 
  mutate(percentage = as.numeric(percentage)) %>% 
  left_join(individuals, by = c("Library" = "IdGenetic")) %>% 
  ggplot(aes(percentage, fill = paste(Genus, Species))) +
  geom_histogram() +
  geom_vline(xintercept = c(33.3, 66.6), linetype = "dashed") +
  scale_fill_discrete("Species") +
  ggtitle("84 libraries < 33% coverage (80.8% above)", "210 libraries < 80% coverage (47.9% above)")
```

## Variant calling

Run the `HaplotypeCaller` on each sample's BAM files to create single-sample gVCFs using the `.g.vcf` extension for the output file.

```{bash HaplotypeCaller, eval=F, echo=T}
for file in $(ls mapping/bam/*.bam); do echo "module load bioinfo/gatk-4.1.2.0 ; gatk --java-options \"-Xmx20G\" HaplotypeCaller -R reference/reference.fasta -I $file -O calling/gvcf/$(basename "${file%.*}").g.vcf.gz  -ERC GVCF"; done > haplo.sh
sarray -J haplo -o out/haplo_%j.out -e out/haplo_%j.err -t 48:00:00 --mem=20G --mail-type=BEGIN,END,FAIL  haplo.sh
```

## Data aggregation

We aggregated the GVCF files and feed in one GVCF database with `GenomicsDBImport` to be genotyped. We divided the step on several intervals files of a maximum of 100 sequences computed in parralel to speed up the operation.

```{bash combine, eval=F, echo=T}
for file in $(ls gvcf*/*.g.vcf.gz) ; do echo -e $(basename "${file%.*}")"\t"$file >> sample_map.txt ; done
mkdir reference.sequences.lists
cut ../reference/reference.fasta.fai -f1 > reference.sequences.lists/reference.sequences.list
cd reference.sequences.lists
split -l 100 -d reference.sequences.list reference.sequences_ --additional-suffix=.list
rm reference.sequences.list
for file in $(ls reference.sequences.lists/); do echo "module load bioinfo/gatk-4.1.2.0 ; gatk --java-options \"-Xmx20g -Xms20g\" GenomicsDBImport --genomicsdb-workspace-path db/\"${file%.*}\".DB -L reference.sequences.lists/$file --sample-name-map sample_map.txt --batch-size 10 --consolidate"; done > combine.sh
sarray -J combine -o out/%j.out -e out/%j.err -t 48:00:00 --mem=40G --mail-type=BEGIN,END,FAIL combine.sh
```

## Joint genotyping

We joint-genotyped individuals with `GenotypeGVCFs` on all of them together to create the raw SNP and indel VCFs that are usually emitted by the callers. We divided the step on several intervals files of a maximum of 1000 sequences computed in parralel to speed up the operation (similarly to previous step). **NB**, we tested the pipeline with 3 individual haplotypes and 10 intervals of 100 sequences ran in parrallel; and it took 6 minutes. Consequently with 10 fold more sequences per intervals we may increase to 1H, and the effect of 10 fold more individual haplotypes is hard to assess. Then we merged genotypes of all intervals with `GatherVcfs` from `Picard` in one raw VCF to be filtered.

```{bash genotype, eval=F, echo=T}
mkdir parvicapture.raw.vcf.gz
for file in $(ls reference.sequences.lists); do echo "module load bioinfo/gatk-4.1.2.0 ; gatk --java-options \"-Xmx20g\" GenotypeGVCFs -R ../reference/reference.fasta -L reference.sequences.lists/$file -V gendb://db/${file%.*}.DB -O parvicapture.raw.vcf.gz/${file%.*}.vcf.gz"; done > genotype.sh
sarray -J genotype -o out/%j.out -e out/%j.err -t 48:00:00 --mem=20G --mail-type=BEGIN,END,FAIL genotype.sh
echo -e '#!/bin/bash\n#SBATCH --time=48:00:00\n#SBATCH -J gather\n#SBATCH -o gather.out\n#SBATCH -e gather.err\n#SBATCH --mem=20G\n#SBATCH --cpus-per-task=1\n#SBATCH --mail-type=BEGIN,END,FAIL\nmodule load bioinfo/picard-2.14.1\njava -Xmx20g -jar $PICARD GatherVcfs \' > gather.sh
for file in $(ls parvicapture.raw.vcf/*.gz)
do
	echo -e '\tI='$file' \' >> gather.sh
done
echo -e '\tO=parvicapture.raw.vcf.gz\n' >> gather.sh
```
