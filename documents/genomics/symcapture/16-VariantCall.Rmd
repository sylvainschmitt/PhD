```{r setup_variant, include=FALSE}
rm(list = ls()) ; invisible(gc()) ; set.seed(42)
library(knitr)
library(kableExtra)
if(knitr:::is_html_output()) options(knitr.table.format = "html") 
if(knitr:::is_latex_output()) options(knitr.table.format = "latex") 
library(tidyverse)
theme_set(bayesplot::theme_default())
opts_chunk$set(
  echo = F, message = F, warning = F, fig.height = 6, fig.width = 8,
  cache = T, cache.lazy = F)
path <- "~/Documents/BIOGECO/PhD/data/Symphonia_Paracou/"
```

```{r variant}

```

# Variant call

We used `GATK` as it has apparently similar performance to other variant callers [@Supernat2018] and was more known by Myriam. For that we used the following pipeline:

1. __Variant calling__ Run the `HaplotypeCaller` on each sample's BAM files to create single-sample gVCFs using the `.g.vcf` extension for the output file.
1. __Data aggregation__ Aggregate the GVCF files and feed in one GVCF with `GenomicsDBImport` to be genotyped
1. __Joint genotyping__ Run `GenotypeGVCFs` on all of them together to create the raw SNP and indel VCFs that are usually emitted by the callers.

## Variant calling

Run the `HaplotypeCaller` on each sample's BAM files to create single-sample gVCFs using the `.g.vcf` extension for the output file.  We used `sarray` which is much powerful than `sbatch` in parralel computing.

```{bash HaplotypeCaller, eval=F, echo=T}
# folders
mkdir variantCalling/gvcf4

# test
file=$(ls mapping/bam4/*.bam | head -n 1)
srun --mem=20G --pty bash
module load bioinfo/gatk-4.1.2.0 ; gatk --java-options "-Xmx20G" HaplotypeCaller -R reference/reference.fasta -I $file -O variantCalling/gvcf4/$(basename "${file%.*}").g.vcf.gz  -ERC GVCF
exit
rm variantCalling/gvcf4/*

# sarray
for file in $(ls mapping/bam4/*.bam); do echo "module load bioinfo/gatk-4.1.2.0 ; gatk --java-options \"-Xmx20G\" HaplotypeCaller -R reference/reference.fasta -I $file -O variantCalling/gvcf4/$(basename "${file%.*}").g.vcf.gz  -ERC GVCF"; done > haplo4.sh
mkdir haplo4
sarray -J haplo4 -o haplo4/%j.out -e haplo4/%j.err -t 48:00:00 --mem=20G --mail-type=BEGIN,END,FAIL  haplo4.sh

# clean
rm -r haplo4
rm -r tmp
```

## Data aggregation

We aggregated the GVCF files and feed in one GVCF database with `GenomicsDBImport` to be genotyped. **Beware**, `GATK 4.0.0.0` does not deal with multiple intervals when using `GenomicsDBImport`, so we used `GATK 4.1.2.0`. We divided the step on several intervals files of a maximum of 1000 sequences computed in parralel to speed up the operation. **NB**, we tested the pipeline with 3 individual haplotypes and 10 intervals of 100 sequences ran in parrallel; and it took 24 minutes. Consequently with 10 fold more sequences per intervals we may increase to 4H, and the effect of 10 fold more individual haplotypes is hard to assess.

```{bash combine, eval=F, echo=T}
# Sample map
touch sample_map.txt
for file in $(ls gvcf.test/*.g.vcf.gz)
do 
   echo -e $(basename "${file%.*}")"\t"$file >> sample_map.txt
done

# seq lists
mkdir reference.sequences.lists
cut ../reference/reference.fasta.fai -f1 > reference.sequences.lists/reference.sequences.list
cd reference.sequences.lists
split -l 1000 -d reference.sequences.list reference.sequences_ --additional-suffix=.list
rm reference.sequences.list
ls | wc -l
cd ..

# folders
mkdir tmp
mkdir symcaptureDB

# test
file=$(ls reference.sequences.lists/ | head -n 1)
srun --mem=20G --pty bash
module load bioinfo/gatk-4.1.2.0 ; gatk --java-options "-Xmx20g -Xms20g" GenomicsDBImport --genomicsdb-workspace-path symcaptureDB/"${file%.*}".DB -L reference.sequences.lists/$file --sample-name-map sample_map.txt --batch-size 50 --tmp-dir=tmp
exit
rm -r symcaptureDB/*
rm tmp/*

# sarray

for file in $(ls reference.sequences.lists.test/); do echo "module load bioinfo/gatk-4.1.2.0 ; gatk --java-options \"-Xmx20g -Xms20g\" GenomicsDBImport --genomicsdb-workspace-path symcaptureDB/\"${file%.*}\".DB -L reference.sequences.lists/$file --sample-name-map sample_map.txt --batch-size 50 --tmp-dir=tmp"; done > combine.array.sh
mkdir combine.array
sarray -J combine -o combine.array/%j.out -e combine.array/%j.err -t 48:00:00 --mem=20G --mail-type=BEGIN,END,FAIL combine.array.sh

# clean
rm -r combine.array
rm -r tmp
```



## Joint genotyping

We joint-genotyped individuals with `GenotypeGVCFs` on all of them together to create the raw SNP and indel VCFs that are usually emitted by the callers. We divided the step on several intervals files of a maximum of 1000 sequences computed in parralel to speed up the operation (similarly to previous step). **NB**, we tested the pipeline with 3 individual haplotypes and 10 intervals of 100 sequences ran in parrallel; and it took 6 minutes. Consequently with 10 fold more sequences per intervals we may increase to 1H, and the effect of 10 fold more individual haplotypes is hard to assess. Then we merged genotypes of all intervals with `GatherVcfs` from `Picard` in one raw VCF to be filtered.

```{bash genotype, eval=F, echo=T}
# folders
mkdir tmp
mkdir symcapture.vcf.gz

# test
file=$(ls reference.sequences.lists/ | head -n 1)
srun --mem=20G --pty bash
module load bioinfo/gatk-4.1.2.0 ; gatk --java-options "-Xmx20g" GenotypeGVCFs -R ../reference/reference.fasta -L reference.sequences.lists/$file -V gendb://symcaptureDB/$file -O symcapture.vcf.gz/"${file%.*}".vcf.gz --tmp-dir=tmp
exit
rm tmp/*
rm symcapture.vcf.gz/*

# sarray

for file in $(ls reference.sequences.lists); do echo "module load bioinfo/gatk-4.1.2.0 ; gatk --java-options \"-Xmx20g\" GenotypeGVCFs -R ../reference/reference.fasta -L reference.sequences.lists/$file -V gendb://symcaptureDB/$file -O symcapture.vcf.gz/\"${file%.*}\".vcf.gz --tmp-dir=tmp"; done > genotype.array.sh
mkdir genotype.array
sarray -J genotype -o genotype.array/%j.out -e genotype.array/%j.err -t 48:00:00 --mem=20G --mail-type=BEGIN,END,FAIL genotype.array.sh

# clean
rm -r genotype.array
rm -r tmp

# merge
echo -e '#!/bin/bash\n#SBATCH --time=48:00:00\n#SBATCH -J gather\n#SBATCH -o gather.out\n#SBATCH -e gather.err\n#SBATCH --mem=20G\n#SBATCH --cpus-per-task=1\n#SBATCH --mail-type=BEGIN,END,FAIL\nmodule load bioinfo/picard-2.14.1\njava -Xmx20g -jar $PICARD GatherVcfs \' > gather.sh
for file in $(ls symcapture.vcf.gz/*.gz)
do
	echo -e '\tI='$file' \' >> gather.sh
done
echo -e '\tO=symcapture.all.raw.vcf.gz\n' >> gather.sh
```

## Result

```{r genomic, eval=F}
# tutorial from https://grunwaldlab.github.io/Population_Genetics_in_R/analysis_of_genome.html
library(vcfR)
symcapture.vcf <- read.vcfR(file.path(path, "Sequences", "variantCalling", "symcapture.all.raw.vcf.gz"))
symcapture.gl <- vcfR2genlight(symcapture.vcf)
library(adegenet)
ploidy(symcapture.gl) <- 2
symcapture.diff <- genetic_diff(symcapture.vcf, pops = as.factor(symcapture.gl@ind.names), method = 'nei')
knitr::kable(head(symcapture.diff[1:13,]))
reshape2::melt(symcapture.diff[,c(3:8)], varnames=c('Index', 'Sample'), value.name = 'Depth', na.rm=TRUE) %>% 
ggplot(aes(x = variable, y = Depth)) + 
  geom_violin(fill="#2ca25f", adjust = 1.2) +
  xlab("") + ylab("")
```


```{r GBS, eval=F}
# tutorial from https://grunwaldlab.github.io/Population_Genetics_in_R/gbs_analysis.html
library(vcfR)
symcapture.vcf <- read.vcfR(file.path(path, "Sequences", "variantCalling", "symcapture.all.raw.vcf.gz"))
symcapture.gl <- vcfR2genlight(symcapture.vcf)
library(poppr)
ploidy(symcapture.gl) <- 2
tree <- aboot(symcapture.gl, tree = "upgma", distance = bitwise.dist, 
              sample = 100, showtree = F, cutoff = 50, quiet = T)
library(ape)
plot.phylo(tree, cex = 0.8, font = 2, adj = 0)
nodelabels(tree$node.label, adj = c(1.3, -0.5), frame = "n", cex = 0.8,font = 3, xpd = TRUE)
axis(side = 1)
title(xlab = "Genetic distance (proportion of loci that are different)")
symcapture.pca <- glPca(symcapture.gl, nf = 3)
barplot(100*symcapture.pca$eig/sum(symcapture.pca$eig), col = heat.colors(50), main="PCA Eigenvalues")
title(ylab="Percent of variance\nexplained", line = 2)
title(xlab="Eigenvalues", line = 1)
ggplot(as.data.frame(symcapture.pca$scores), aes(x = PC1, y = PC2)) + 
  geom_point(size=2) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0)
```

